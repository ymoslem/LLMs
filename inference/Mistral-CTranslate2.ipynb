{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ymoslem/LLMs/blob/main/inference/Mistral-CTranslate2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PYTSbjuwmdV"
      },
      "source": [
        "# Translation with Mistral 7B using CTranslate2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Zxr4HGJT4LX0"
      },
      "outputs": [],
      "source": [
        "!pip3 install CTranslate2 transformers -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcHklgkgUCXu"
      },
      "outputs": [],
      "source": [
        "# Google Colab switched to CUDA 12 while CTranslate2 still uses CUDA 11\n",
        "# RuntimeError: Library libcublas.so.11 is not found or cannot be loaded\n",
        "# If you received this error during translation, try to install libcublas11\n",
        "\n",
        "# !apt install libcublas11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "C-1sI-17ADLJ",
        "outputId": "54f1da87-59ff-4e66-b924-ee40e25437da"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "shared_drive = userdata.get(\"shared_drive\")\n",
        "\n",
        "directory = os.path.join(shared_drive, \"models\")\n",
        "\n",
        "os.chdir(directory)\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZ28kfD5BT-F"
      },
      "outputs": [],
      "source": [
        "# Convert Mistral to the CTranslate2 format, if you did not already\n",
        "# !ct2-transformers-converter --model mistralai/Mistral-7B-v0.1 --quantization int8 --output_dir ct2-mistral-7B-v0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vI47hGrlYN1N"
      },
      "source": [
        "## Load the model\n",
        "\n",
        "```python\n",
        "model = \"ct2-mistral-7B-v0.1\"\n",
        "tokenizer = \"mistralai/Mistral-7B-v0.1\"\n",
        "```\n",
        "\n",
        "Other models:\n",
        "```\n",
        "model = \"ct2-mistral-7B-instruct-v0.1\"\n",
        "tokenizer = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "model = \"ct2-falcon-7b-instruct\"\n",
        "tokenizer = \"tiiuae/falcon-40b-instruct\"\n",
        "\n",
        "model = \"ct2-zephyr-7b-beta\"\n",
        "tokenizer = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Ln6SAXa77g7z"
      },
      "outputs": [],
      "source": [
        "# Load the model\n",
        "\n",
        "import ctranslate2\n",
        "import transformers\n",
        "\n",
        "model = \"ct2-mistral-7B-v0.1\"\n",
        "tokenizer = \"mistralai/Mistral-7B-v0.1\"\n",
        "\n",
        "# Load the translator and tokenizer\n",
        "generator = ctranslate2.Generator(model, device=\"cuda\")\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCmd0lC98GQB",
        "outputId": "0f0f9351-0561-46a7-ad20-7d844c3f4ca0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spanish: Período de validez después de abierto el envase: 10 horas.\n",
            "English:\n",
            "\n",
            "Spanish: Período de validez después de abierto el envase: 4 semanas\n",
            "English: Shelf life after opening the immediate packaging: 4 weeks.\n",
            "Spanish: Período de validez después de abierto el envase: 10 horas.\n",
            "English:\n"
          ]
        }
      ],
      "source": [
        "# Test prompts\n",
        "\n",
        "src_lang = \"Spanish\"\n",
        "tgt_lang = \"English\"\n",
        "\n",
        "# Zero-shot prompt\n",
        "prompt_source = (\n",
        "    f\"{src_lang}: Período de validez después de abierto el envase: 10 horas.\\n\"\n",
        "    f\"{tgt_lang}:\"\n",
        ")\n",
        "\n",
        "# Fuzzy one-shot prompt\n",
        "prompt_fuzzy = (\n",
        "    f\"{src_lang}: Período de validez después de abierto el envase: 4 semanas\\n\"\n",
        "    f\"{tgt_lang}: Shelf life after opening the immediate packaging: 4 weeks.\\n\"\n",
        "    f\"{src_lang}: Período de validez después de abierto el envase: 10 horas.\\n\"\n",
        "    f\"{tgt_lang}:\"\n",
        ")\n",
        "\n",
        "prompts = [prompt_source, prompt_fuzzy]\n",
        "\n",
        "print(*prompts, sep=\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PUyR0h7vOtSy"
      },
      "outputs": [],
      "source": [
        "# Add stopping criteria to avoid over-generation\n",
        "# References:\n",
        "# https://github.com/OpenNMT/CTranslate2/issues/1309\n",
        "# https://github.com/OpenNMT/CTranslate2/issues/1322\n",
        "# https://stackoverflow.com/questions/69403613/how-to-early-stop-autoregressive-model-with-a-list-of-stop-words\n",
        "\n",
        "stopping_criteria = tokenizer.convert_ids_to_tokens(tokenizer.encode(\".\\n\"))\n",
        "# Probably also re-add the default end of sentence token, but maybe it is not nescessary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZsLGH3l5pxY",
        "outputId": "3ea1e78d-d25b-4630-a6c8-f8ca8dc3f137"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "max_length=44\n",
            "output_length=14\n",
            "\n",
            "Translation:\n",
            "Period of validity after opening the package: 10 hours.\n"
          ]
        }
      ],
      "source": [
        "# Tokenize and generate (single prompt)\n",
        "\n",
        "prompt = prompts[0]\n",
        "\n",
        "max_length = len(prompt.split(\"\\n\")[-2].split(\" \")) * 4\n",
        "\n",
        "\n",
        "tokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(prompt))\n",
        "\n",
        "results = generator.generate_batch([tokens],\n",
        "                                   sampling_topk=1,  # 1 for greed search\n",
        "                                   max_length=max_length,\n",
        "                                   include_prompt_in_result=False,\n",
        "                                   end_token=stopping_criteria\n",
        "                                   )\n",
        "output_ids = results[0].sequences_ids[0]\n",
        "output = tokenizer.decode(output_ids)\n",
        "\n",
        "output_length = len(output_ids)\n",
        "print(f\"{max_length=}\")\n",
        "print(f\"{output_length=}\")\n",
        "\n",
        "print(f\"\\nTranslation:\\n{output}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsojUYhja8vo",
        "outputId": "73acda93-e229-4990-ae2a-75647fc5dc4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "max_length=40\n"
          ]
        }
      ],
      "source": [
        "longest_prompt = max([len(prompt.split(\"\\n\")[-2].split(\" \")[1:]) for prompt in prompts])\n",
        "max_length = longest_prompt * 4\n",
        "print(f\"{max_length=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WT7kGz5UZgpe",
        "outputId": "6110c67f-4c34-4ff3-f5c7-6dbd79e135c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Period of validity after opening the package: 10 hours.\n",
            "\n",
            "Shelf life after opening the immediate packaging: 10 hours.\n"
          ]
        }
      ],
      "source": [
        "# Tokenize the prompts (batch)\n",
        "tokenized_inputs = tokenizer(prompts)\n",
        "\n",
        "# Extract the token IDs for the batch\n",
        "input_ids_batch = tokenized_inputs['input_ids']\n",
        "\n",
        "# Convert the batch of token IDs to tokens\n",
        "tokens_batch = [tokenizer.convert_ids_to_tokens(ids) for ids in input_ids_batch]\n",
        "\n",
        "# Generate outputs in a batch\n",
        "results = generator.generate_batch(tokens_batch,\n",
        "                                   sampling_topk=1,  # 1 for greedy search\n",
        "                                   max_length=max_length,\n",
        "                                   include_prompt_in_result=False,\n",
        "                                   end_token=stopping_criteria\n",
        "                                  )\n",
        "\n",
        "# Decode the outputs\n",
        "outputs = [tokenizer.decode(output_ids) for result in results for output_ids in result.sequences_ids]\n",
        "\n",
        "# 'outputs' will now contain the generated text for each sentence in your list\n",
        "print(*outputs, sep=\"\\n\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOJ/tY7ZkObocc/yqXcQZrY",
      "gpuType": "A100",
      "include_colab_link": true,
      "machine_shape": "hm",
      "mount_file_id": "1wybKipudTNY8_9wMO6YqiNQrlNUglLs1",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
