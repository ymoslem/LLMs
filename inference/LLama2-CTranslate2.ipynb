{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1ihEx-bARyBO6fZwBO16e04d9-tBsNsVs",
      "authorship_tag": "ABX9TyOY/IKF7P+daz350lZeNC5o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ymoslem/LLMs/blob/main/inference/LLama2-CTranslate2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLama 2 with CTranslate2\n",
        "\n",
        "1. To use Llama2, you must first request access [here](https://huggingface.co/meta-llama/Llama-2-70b-chat-hf).\n",
        "2. You will also have to agree to the license [here](https://ai.meta.com/resources/models-and-libraries/llama-downloads/).\n",
        "3. When you receive the email from step #2, follow the instructions to download \"tokenizer.model\" too if you want to use SentencePiece directly; otherwise, you can use Hugging Face's `LlamaTokenizerFast`\n",
        "4. Finally, obtain a Hugging Face access token [here](https://huggingface.co/settings/tokens)."
      ],
      "metadata": {
        "id": "NsjNy7V0-LyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install accelerate ctranslate2 sentencepiece transformers einops &> /dev/null"
      ],
      "metadata": {
        "id": "SrFzZDmo-NPh"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Log in to huggingface - change the access token\n",
        "\n",
        "!huggingface-cli login --token \"hf_your_access_token\""
      ],
      "metadata": {
        "id": "aGTFL95qP1R0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [Recommended] Save your files in Google Drive\n",
        "\n",
        "# !mkdir -p /content/drive/MyDrive/models/ct2-meta-llama\n",
        "# %cd /content/drive/MyDrive/models/ct2-meta-llama"
      ],
      "metadata": {
        "id": "XdzjSoV3x-Fs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert LLama to the CTranslate2 format (2mins for Llama-2-7b-chat-hf)\n",
        "# Model names: meta-llama/Llama-2-{7,13,70}b-chat-hf\n",
        "\n",
        "!ct2-transformers-converter --model meta-llama/Llama-2-13b-chat-hf --quantization int8 --output_dir ct2-llama-2-13b-chat-hf --trust_remote_code --low_cpu_mem_usage\n"
      ],
      "metadata": {
        "id": "YZ1inHhuLZKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can Convert Falcon in the same way\n",
        "\n",
        "# !ct2-transformers-converter --model tiiuae/falcon-7b-instruct --quantization int8 --output_dir ct2-falcon-7b-instruct --trust_remote_code"
      ],
      "metadata": {
        "id": "O4TQSlhC-See"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "\n",
        "import ctranslate2\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# The path of the model you just converted to the CTranslate2\n",
        "model_path = \"ct2-llama-2-13b-chat-hf\"\n",
        "\n",
        "generator = ctranslate2.Generator(model_path, device=device)"
      ],
      "metadata": {
        "id": "pQZ1pS5bF-CT"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer\n",
        "\n",
        "from transformers import LlamaTokenizerFast\n",
        "\n",
        "model_name = \"meta-llama/Llama-2-13b-chat-hf\"\n",
        "\n",
        "tokenizer = LlamaTokenizerFast.from_pretrained(model_name,\n",
        "                                              padding_side=\"left\")\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "XqmQ13df978q"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [Optional] save the tokenizer to use later\n",
        "# tokenizer.save_pretrained(\"./llama2_tokenizer/\")"
      ],
      "metadata": {
        "id": "H6BV5XgEVStR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 1: Translation"
      ],
      "metadata": {
        "id": "Ofeyy_Zd_iBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt 1\n",
        "\n",
        "src = \"Spanish\"\n",
        "tgt = \"English\"\n",
        "\n",
        "sentences = [\"Tom tiene dos hermanos que viven en Boston.\",\n",
        "            \"La isla está deshabitada.\",\n",
        "            \"La escuela se encuentra en la colina.\"]\n",
        "\n",
        "query = f\"Translate the following text from {src} to {tgt}:\"\n",
        "\n",
        "prompts = [f\"{query}\\n{src}: {sent}\\n{tgt}:\" for sent in sentences]\n",
        "\n",
        "print(*prompts, sep=\"\\n\\n\")"
      ],
      "metadata": {
        "id": "UuYrFEC6_MCL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e675e49c-8625-4a03-bac1-7398109991ae"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translate the following text from Spanish to English:\n",
            "Spanish: Tom tiene dos hermanos que viven en Boston.\n",
            "English:\n",
            "\n",
            "Translate the following text from Spanish to English:\n",
            "Spanish: La isla está deshabitada.\n",
            "English:\n",
            "\n",
            "Translate the following text from Spanish to English:\n",
            "Spanish: La escuela se encuentra en la colina.\n",
            "English:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt 2\n",
        "\n",
        "src = \"Spanish\"\n",
        "tgt = \"English\"\n",
        "\n",
        "sentences = [\"Tom tiene dos hermanos que viven en Boston.\",\n",
        "            \"La isla está deshabitada.\",\n",
        "            \"La escuela se encuentra en la colina.\"]\n",
        "\n",
        "query = f\"Translate the following text from {src} to {tgt}:\"\n",
        "\n",
        "prompts = [f\"{query}\\n{sent}\\n{tgt}:\" for sent in sentences]\n",
        "\n",
        "print(*prompts, sep=\"\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dq-bscUv64mW",
        "outputId": "a175732b-a9c6-483e-fbea-d7f93f8a2b10"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translate the following text from Spanish to English:\n",
            "Tom tiene dos hermanos que viven en Boston.\n",
            "English:\n",
            "\n",
            "Translate the following text from Spanish to English:\n",
            "La isla está deshabitada.\n",
            "English:\n",
            "\n",
            "Translate the following text from Spanish to English:\n",
            "La escuela se encuentra en la colina.\n",
            "English:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the prompts\n",
        "\n",
        "import torch\n",
        "\n",
        "tokens = [tokenizer.convert_ids_to_tokens(tokenizer.encode(prompt)) for prompt in prompts]"
      ],
      "metadata": {
        "id": "ELxHPTVA9m3f"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(*tokens, sep=\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8edGX5p4D70U",
        "outputId": "7e4ac640-060e-4f33-9204-7c96aa1521f6"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<s>', '▁Trans', 'late', '▁the', '▁following', '▁text', '▁from', '▁Spanish', '▁to', '▁English', ':', '<0x0A>', 'Tom', '▁tiene', '▁dos', '▁h', 'erman', 'os', '▁que', '▁v', 'iven', '▁en', '▁Boston', '.', '<0x0A>', 'English', ':']\n",
            "['<s>', '▁Trans', 'late', '▁the', '▁following', '▁text', '▁from', '▁Spanish', '▁to', '▁English', ':', '<0x0A>', 'La', '▁is', 'la', '▁está', '▁des', 'hab', 'it', 'ada', '.', '<0x0A>', 'English', ':']\n",
            "['<s>', '▁Trans', 'late', '▁the', '▁following', '▁text', '▁from', '▁Spanish', '▁to', '▁English', ':', '<0x0A>', 'La', '▁esc', 'uela', '▁se', '▁encuentra', '▁en', '▁la', '▁col', 'ina', '.', '<0x0A>', 'English', ':']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate outputs\n",
        "\n",
        "results = generator.generate_batch(tokens,\n",
        "                                   sampling_topk=10,\n",
        "                                   max_length=20,\n",
        "                                   include_prompt_in_result=False,\n",
        "                                   min_length=3,\n",
        "                                  )\n",
        "outputs = [tokenizer.decode(result.sequences_ids[0]).strip() for result in results]\n",
        "\n",
        "# Remove over-generation\n",
        "clean_outputs = [output.split(\"\\n\")[0] for output in outputs]\n",
        "\n",
        "print(*clean_outputs, sep=\"\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnAH9yuGYx1b",
        "outputId": "3165cb34-9bf3-48c9-da87-f62f8c7c438b"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tom has two brothers who live in Boston.\n",
            "\n",
            "The island is uninhabited.\n",
            "\n",
            "The school is located on the hill.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 2: Summarization"
      ],
      "metadata": {
        "id": "ItJJ6RB5_l_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = f\"Summarize the following paper:\"\n",
        "\n",
        "abstracts = [\n",
        "    \"\"\"Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different \"thinned\" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.\"\"\",\n",
        "    \"\"\"Consistency is a key requirement of high-quality translation. It is especially important to adhere to pre-approved terminology and adapt to corrected translations in domain-specific projects. Machine translation (MT) has achieved significant progress in the area of domain adaptation. However, real-time adaptation remains challenging. Large-scale language models (LLMs) have recently shown interesting capabilities of in-context learning, where they learn to replicate certain input-output text generation patterns, without further fine-tuning. By feeding an LLM at inference time with a prompt that consists of a list of translation pairs, it can then simulate the domain and style characteristics. This work aims to investigate how we can utilize in-context learning to improve real-time adaptive MT. Our extensive experiments show promising results at translation time. For example, LLMs can adapt to a set of in-domain sentence pairs and/or terminology while translating a new sentence. We observe that the translation quality with few-shot in-context learning can surpass that of strong encoder-decoder MT systems, especially for high-resource languages. Moreover, we investigate whether we can combine MT from strong encoder-decoder models with fuzzy matches, which can further improve translation quality, especially for less supported languages. We conduct our experiments across five diverse language pairs, namely English-to-Arabic (EN-AR), English-to-Chinese (EN-ZH), English-to-French (EN-FR), English-to-Kinyarwanda (EN-RW), and English-to-Spanish (EN-ES).\"\"\"\n",
        "    ]\n",
        "\n",
        "authors = [\"Srivastava at al. (2014)\",\n",
        "           \"Moslem et al. (2023)\"]\n",
        "\n",
        "prompts = [f\"{query}\\nAbstract: {abstract}\\nSummary: {author}\" for abstract, author in zip(abstracts, authors)]\n",
        "\n",
        "print(*prompts, sep=\"\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkXVZQ1E_puJ",
        "outputId": "9243a259-111f-4aec-e709-60a5d20824b5"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summarize the following paper:\n",
            "Abstract: Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different \"thinned\" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.\n",
            "Summary: Srivastava at al. (2014)\n",
            "\n",
            "Summarize the following paper:\n",
            "Abstract: Consistency is a key requirement of high-quality translation. It is especially important to adhere to pre-approved terminology and adapt to corrected translations in domain-specific projects. Machine translation (MT) has achieved significant progress in the area of domain adaptation. However, real-time adaptation remains challenging. Large-scale language models (LLMs) have recently shown interesting capabilities of in-context learning, where they learn to replicate certain input-output text generation patterns, without further fine-tuning. By feeding an LLM at inference time with a prompt that consists of a list of translation pairs, it can then simulate the domain and style characteristics. This work aims to investigate how we can utilize in-context learning to improve real-time adaptive MT. Our extensive experiments show promising results at translation time. For example, LLMs can adapt to a set of in-domain sentence pairs and/or terminology while translating a new sentence. We observe that the translation quality with few-shot in-context learning can surpass that of strong encoder-decoder MT systems, especially for high-resource languages. Moreover, we investigate whether we can combine MT from strong encoder-decoder models with fuzzy matches, which can further improve translation quality, especially for less supported languages. We conduct our experiments across five diverse language pairs, namely English-to-Arabic (EN-AR), English-to-Chinese (EN-ZH), English-to-French (EN-FR), English-to-Kinyarwanda (EN-RW), and English-to-Spanish (EN-ES).\n",
            "Summary: Moslem et al. (2023)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the prompts\n",
        "\n",
        "import torch\n",
        "\n",
        "tokens = [tokenizer.convert_ids_to_tokens(tokenizer.encode(prompt)) for prompt in prompts]"
      ],
      "metadata": {
        "id": "mLeAiydwB1uX"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokens[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-m4CwVLDN0L",
        "outputId": "9c817491-4737-41ef-da08-13007bef7493"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<s>', '▁Sum', 'mar', 'ize', '▁the', '▁following', '▁paper', ':', '<0x0A>', 'Abstract', ':', '▁Deep', '▁neural', '▁n', 'ets', '▁with', '▁a', '▁large', '▁number', '▁of', '▁parameters', '▁are', '▁very', '▁powerful', '▁machine', '▁learning', '▁systems', '.', '▁However', ',', '▁over', 'f', 'itting', '▁is', '▁a', '▁serious', '▁problem', '▁in', '▁such', '▁networks', '.', '▁Lar', 'ge', '▁networks', '▁are', '▁also', '▁slow', '▁to', '▁use', ',', '▁making', '▁it', '▁difficult', '▁to', '▁deal', '▁with', '▁over', 'f', 'itting', '▁by', '▁combining', '▁the', '▁predictions', '▁of', '▁many', '▁different', '▁large', '▁neural', '▁n', 'ets', '▁at', '▁test', '▁time', '.', '▁Drop', 'out', '▁is', '▁a', '▁technique', '▁for', '▁address', 'ing', '▁this', '▁problem', '.', '▁The', '▁key', '▁idea', '▁is', '▁to', '▁randomly', '▁drop', '▁units', '▁(', 'al', 'ong', '▁with', '▁their', '▁connections', ')', '▁from', '▁the', '▁neural', '▁network', '▁during', '▁training', '.', '▁This', '▁prevents', '▁units', '▁from', '▁co', '-', 'ada', 'pt', 'ing', '▁too', '▁much', '.', '▁During', '▁training', ',', '▁drop', 'out', '▁samples', '▁from', '▁an', '▁exponential', '▁number', '▁of', '▁different', '▁\"', 'th', 'inned', '\"', '▁networks', '.', '▁At', '▁test', '▁time', ',', '▁it', '▁is', '▁easy', '▁to', '▁approximate', '▁the', '▁effect', '▁of', '▁aver', 'aging', '▁the', '▁predictions', '▁of', '▁all', '▁these', '▁th', 'inned', '▁networks', '▁by', '▁simply', '▁using', '▁a', '▁single', '▁un', 'th', 'inned', '▁network', '▁that', '▁has', '▁smaller', '▁weights', '.', '▁This', '▁significantly', '▁reduces', '▁over', 'f', 'itting', '▁and', '▁gives', '▁major', '▁improvements', '▁over', '▁other', '▁regular', 'ization', '▁methods', '.', '▁We', '▁show', '▁that', '▁drop', 'out', '▁impro', 'ves', '▁the', '▁performance', '▁of', '▁neural', '▁networks', '▁on', '▁super', 'vised', '▁learning', '▁tasks', '▁in', '▁vision', ',', '▁speech', '▁recognition', ',', '▁document', '▁classification', '▁and', '▁computational', '▁bi', 'ology', ',', '▁obtain', 'ing', '▁state', '-', 'of', '-', 'the', '-', 'art', '▁results', '▁on', '▁many', '▁benchmark', '▁data', '▁sets', '.', '<0x0A>', 'Summary', ':', '▁S', 'riv', 'ast', 'ava', '▁at', '▁al', '.', '▁(', '2', '0', '1', '4', ')']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate outputs\n",
        "\n",
        "results = generator.generate_batch(tokens,\n",
        "                                   sampling_topk=15,\n",
        "                                   max_length=200,\n",
        "                                   include_prompt_in_result=False,\n",
        "                                   min_length=50,\n",
        "                                  )\n",
        "outputs = [tokenizer.decode(result.sequences_ids[0]).strip() for result in results]\n",
        "final_outputs = [f\"{author} {output}\" for author, output in zip(authors, outputs)]\n",
        "\n",
        "print(*final_outputs, sep=\"\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4y9nkDCoB3wO",
        "outputId": "77755ded-fa3e-4b9e-dbce-2e073b8618b1"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Srivastava at al. (2014) propose a technique called \"dropout\" to prevent overfitting in deep neural networks. The key idea is to randomly drop units and their connections during training, preventing co-adaptation and reducing overfitting. The authors show that dropout improves performance on various tasks in computer vision, speech recognition, document classification, and bioinformatics, achieving state-of-the-art results on many benchmark datasets. The paper provides a simple and efficient way to reduce overfitting in deep neural networks, and has had a significant impact on the field of machine learning.\n",
            "\n",
            "Moslem et al. (2023) explore the use of in-context learning in large language models (LLMs) to improve real-time adaptive machine translation (MT). The authors investigate whether LLMs can adapt to a set of in-domain sentence pairs and/or terminology while translating a new sentence. The results show that the translation quality with few-shot in-context learning can surpass that of strong encoder-decoder MT systems, especially for high-resource languages. The paper also explores the combination of MT from strong encoder-decoder models with fuzzy matches to further improve translation quality, especially for less supported languages. The experiments are conducted across five diverse language pairs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 3: Using SentencePiece directly"
      ],
      "metadata": {
        "id": "rIvveCJZghCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "sp = spm.SentencePieceProcessor(\"tokenizer.model\")\n",
        "\n",
        "sp.encode_as_pieces(\"This is a test\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrKZRHGdgmKC",
        "outputId": "56dd5671-3306-4698-c9db-b9aca146777c"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['▁This', '▁is', '▁a', '▁test']"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = sp.encode_as_pieces(prompts)\n",
        "tokens = [['<s>'] + prompt for prompt in tokens]"
      ],
      "metadata": {
        "id": "YCOB6oGUguPk"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokens[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Uf9dVithPWI",
        "outputId": "195293a8-3383-4fc0-a172-108f8d711e4a"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<s>', '▁Sum', 'mar', 'ize', '▁the', '▁following', '▁paper', ':', '<0x0A>', 'Abstract', ':', '▁Deep', '▁neural', '▁n', 'ets', '▁with', '▁a', '▁large', '▁number', '▁of', '▁parameters', '▁are', '▁very', '▁powerful', '▁machine', '▁learning', '▁systems', '.', '▁However', ',', '▁over', 'f', 'itting', '▁is', '▁a', '▁serious', '▁problem', '▁in', '▁such', '▁networks', '.', '▁Lar', 'ge', '▁networks', '▁are', '▁also', '▁slow', '▁to', '▁use', ',', '▁making', '▁it', '▁difficult', '▁to', '▁deal', '▁with', '▁over', 'f', 'itting', '▁by', '▁combining', '▁the', '▁predictions', '▁of', '▁many', '▁different', '▁large', '▁neural', '▁n', 'ets', '▁at', '▁test', '▁time', '.', '▁Drop', 'out', '▁is', '▁a', '▁technique', '▁for', '▁address', 'ing', '▁this', '▁problem', '.', '▁The', '▁key', '▁idea', '▁is', '▁to', '▁randomly', '▁drop', '▁units', '▁(', 'al', 'ong', '▁with', '▁their', '▁connections', ')', '▁from', '▁the', '▁neural', '▁network', '▁during', '▁training', '.', '▁This', '▁prevents', '▁units', '▁from', '▁co', '-', 'ada', 'pt', 'ing', '▁too', '▁much', '.', '▁During', '▁training', ',', '▁drop', 'out', '▁samples', '▁from', '▁an', '▁exponential', '▁number', '▁of', '▁different', '▁\"', 'th', 'inned', '\"', '▁networks', '.', '▁At', '▁test', '▁time', ',', '▁it', '▁is', '▁easy', '▁to', '▁approximate', '▁the', '▁effect', '▁of', '▁aver', 'aging', '▁the', '▁predictions', '▁of', '▁all', '▁these', '▁th', 'inned', '▁networks', '▁by', '▁simply', '▁using', '▁a', '▁single', '▁un', 'th', 'inned', '▁network', '▁that', '▁has', '▁smaller', '▁weights', '.', '▁This', '▁significantly', '▁reduces', '▁over', 'f', 'itting', '▁and', '▁gives', '▁major', '▁improvements', '▁over', '▁other', '▁regular', 'ization', '▁methods', '.', '▁We', '▁show', '▁that', '▁drop', 'out', '▁impro', 'ves', '▁the', '▁performance', '▁of', '▁neural', '▁networks', '▁on', '▁super', 'vised', '▁learning', '▁tasks', '▁in', '▁vision', ',', '▁speech', '▁recognition', ',', '▁document', '▁classification', '▁and', '▁computational', '▁bi', 'ology', ',', '▁obtain', 'ing', '▁state', '-', 'of', '-', 'the', '-', 'art', '▁results', '▁on', '▁many', '▁benchmark', '▁data', '▁sets', '.', '<0x0A>', 'Summary', ':', '▁S', 'riv', 'ast', 'ava', '▁at', '▁al', '.', '▁(', '2', '0', '1', '4', ')']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate outputs\n",
        "\n",
        "results = generator.generate_batch(tokens,\n",
        "                                   sampling_topk=15,\n",
        "                                   max_length=200,\n",
        "                                   include_prompt_in_result=False,\n",
        "                                   min_length=50,\n",
        "                                  )\n",
        "outputs = [tokenizer.decode(result.sequences_ids[0]).strip() for result in results]\n",
        "final_outputs = [f\"{author} {output}\" for author, output in zip(authors, outputs)]\n",
        "\n",
        "print(*final_outputs, sep=\"\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enTyFqbfhQUP",
        "outputId": "c2b232e8-daf0-4dfd-d008-97bf6b3ae42e"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Srivastava at al. (2014) introduce dropout as a regularization technique to address overfitting in deep neural networks. The method randomly drops units and their connections during training, preventing co-adaptation and improving generalization. By training a single unthinned network at test time, dropout achieves better results on various supervised learning tasks compared to other regularization methods, leading to state-of-the-art performance on benchmark datasets in vision, speech recognition, document classification, and computational biology.\n",
            "\n",
            "Moslem et al. (2023) investigate the use of in-context learning in large-scale language models (LLMs) to improve real-time adaptive machine translation (MT). The authors feed an LLM with a prompt consisting of a list of translation pairs and observe promising results in adapting to new sentences and terminology in different domains and languages. The study shows that few-shot in-context learning can improve translation quality, especially for high-resource languages. Additionally, the authors explore combining MT from strong encoder-decoder models with fuzzy matches to further enhance translation quality, particularly for less supported languages. The results are promising and indicate the potential of in-context learning for real-time adaptive MT.\n"
          ]
        }
      ]
    }
  ]
}
